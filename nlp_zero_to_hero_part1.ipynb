{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_zero_to_hero_part1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMpIUJ9U2KRl00tMt1Wlbsa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zfukuoka/Copying_a_sutra/blob/master/nlp_zero_to_hero_part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6COsnDoU3pg",
        "colab_type": "text"
      },
      "source": [
        "# 写経元\n",
        "\n",
        "[Natural Language Processing - Tokenization (NLP Zero to Hero - Part 1)](https://www.youtube.com/watch?v=fNxaJsNG3-s&list=PLQY2H8rRoyvwLbzbnKJ59NkZvQAW9wLbx&index=2&t=0s)\n",
        "\n",
        "動画内のリンク\n",
        "\n",
        "* [Colab](https://colab.research.google.com/github/lmoroney/dlaicourse/blob/master/TensorFlow%20In%20Practice/Course%203%20-%20NLP/Course%203%20-%20Week%201%20-%20Lesson%201.ipynb)\n",
        "\n",
        "* [YouTubeのプレイリスト](https://www.youtube.com/playlist?list=PLQY2H8rRoyvzDbLUZkbudP-MFQZwNmU4S)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXRPO-olW7I8",
        "colab_type": "text"
      },
      "source": [
        "## ワードのインデックス化\n",
        "\n",
        "[Tokenizer](https://keras.io/api/preprocessing/text/) を用いて与えた文章に出てくるワードに対して、インデックス化（あるいはベクター化）。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmB8BQEWUgr8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "352629e5-a4ec-4622-e3ea-24c2fee3bcbc"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "sentences = [\n",
        "      'i love my dog',\n",
        "      'I love my cat',\n",
        "      'You love my dog!'\n",
        "]\n",
        "\n",
        "tokenizer = Tokenizer(num_words=100)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}